%pre
  Working on and Submitting Programming Exercises
  
%h3 Classification Problem
%p
  %pre
    h(x) = Theta-transpose X (Theta.T.X)
    This (linear regression) is not a good model for Classification, 0-1 problems.
    Logistic Regression is a classification algo, not a regression algo,
    Hypothesis Representation.
    we want: 0 < h(x) < 1
    in olden case, it was: h(x) = Theta times x
    this time it is: g(theta times x)
    g(z) = 1/(1 + exp(-z))
    this is the sigmoid function
    now in this case, h(x) will always be between 0 and 1, and i will interpret it as the probability that y=1 on input x.
    if x = [x0, x1] = [1 tumorsize], and h(x) = 0.7, i will tell patient he has .7 probability of having malignant tumor.
    that is, for y=1, he has probability 0.7
    h(x) = P(y=1|x;theta)
    probability y=1, given x, parameterized by theta
    since y = 0, or 1, so y = 0 can be calculated easily.
    Decision boundary.
    suppose h(x) = g(theta-0 + theta-1.x1 + theta-2.x2)
    suppose theta-0 = -3, theta-1 = 1, theta-2 = 1
    theta = [-3 1 1]
    predict y = 1 if -3 + x1 + x2 >= 0
    x1 + x2 = 3 is the decision boundary
    Non Linear Decision Boundary
    h(x) = g(theta-0 + theta-1.x1 + theta-2.x2 + theta-3.x1^2 + theta-4.x2^2)
    eg theta = [-1 0 0 1 1] in the above example via a procedure yet to be specified
    now, predict y = 1 if -1 + x1^2 + x2^2 >= 0
    this is equation of a circle radius 1
    you can have more complicated decision boundaries eq:
    h(x) = g(theta-0 + theta-1.x1 + theta-2.x2 + theta-3.x1^2 + theta-4.x1^2.x2 +
    \          theta-5.x1^2.x2^2 + theta-6.x1^3.x2 + ...)
    Next Video: how to automatically choose parameters theta so that given a training set, we can automatically fit the parameters to our data.
    
    Logistic Regression
    Simplified Cost Function and Gradient Descent
    J(theta) = (1/m)(Summation(i in 1..m)Cost(h(theta(xi)), yi))
    Cost(h(theta(x)), y) = if y = 1, -log(h(theta(x)))
    \                       if y = 0, -log(1 - h(theta(x)))
    \                       Note, y is either 0 or 1
    One equation for the above which gets rid of the two equations and is good for gradient descent:
    Cost(h(theta(x)), y) = -ylog(h(theta(x))) - (1-y)log(1-h(theta(x)))
    
    therefore logistic regression cost function can be written as:
    J(theta) = (1/m)(Summation(i in 1..m)Cost(h(theta(xi)), yi))
    \         = (-1/m)(Summation(i in 1..m)yilog(h(theta(xi))) +(1-yi)log(1-h(theta(xi)))  )
    Why this cost function?
    \  In statistics you can derive this as maximum likelihood estimation
    \  it is also convex.
    Now, given this cost function, we need to find the parameters theta that minimize the cost function above,
    
    that is, choose theta such that min(over theta)J(theta)
    Once you have chosen such a theta, then given a new input x,
    you predict y as follows:
    output h(theta(x)) = 1/(1+exp(-theta-transpose times x))
    \                   = 1/(1+exp(-theta.x)) where theta and x are vectors, and you take their dot product.
    also, we are also going to predict the output value as the probability that y = 1.
    P(y=1|x;theta)
    The only task left is to now minimize J(theta) and choose theta in the appropriate way.
    Repeatedly do: {
    theta-j = theta-j - a(d/d(theta-j)J(theta))
    simultaneously update all theta-j
    }
    Now, partial derivative of J(theta) wrt to theta-j, for all j is:
    d of J(theta)/d(theta-j) = (1/m)Summation(i in 1..m) (h(theta(xi)) - yi)x(sub j super i)
    
    so, let theta = [theta-0 theta-1 ... theta-n]
    Repeatedly do: {
    theta-j = theta-j - a(summation(i in 1..m)(h(theta, xi) - y(i))x(j,i))
    }
    The above equation is exactly what we had for linear regression!
    So what has changed? in linear regression, h(x) = theta'.X
    In logistic regression, h(x) = 1/(1+exp(-theta'.X))
    
    Next Video: Logistic Regression
    \            Advanced Optimization.
    The setting is, that we want to compute both J(theta) as well as partial derivatives with respect to each theta-j of J(theta). Gradient descent repeatedly performs theta-j = theta-j - (alpha x d(J)/d(theta-j))
    
    Suppose that given theta, we have code that can compute
    1. J(theta)
    2. d/d(theta-j) J(theta)
    we have the following optimization algorithms:
    1 Gradient Descent
    2 Conjugate Gradient
    3 BFGS
    4 L-BFGS
    The advantages of 2-4 are: no need to manually pick alpha, often faster than gradient descent
    disadvantage: more complicated.
    bascially, they have a good inner loop that picks a good alpha itself, so in each iteration the alpha can be different, so you dont need to manually pick alpha.
    octave indexes from 1.
    
Here is the example code for the above:
%pre
  function [jVal, gradient] = costFunction(theta)
  jVal = (theta(1)-5)^2 + (theta(2)-5)^2;
  gradient = zeros(2,1);
  gradient(1) = 2*(theta(1)-5);
  gradient(2) = 2*(theta(2)-5);
  \% Basically, J(theta) = (theta-1 - 5)^2 + (theta-2 - 5)^2
  \% Partial derivative wrt theta-1: 2(theta-1 - 5), theta-2: 2(theta-2 - 5) 
  \% we know the minimum happens if theta-1 = theta-2 = 5
  \% What this function does is: returns two values, one is the J(theta) or the value of the cost function
  \% the other is: it returns the gradient at the particular value of theta, so if u tell it theta-1 and theta-2, it 
  \% will tell the value of the gradient of J(theta) wrt theta-1, theta-2 at the value of theta specified.
  \% u call it like this: 
  \%% options = optimset('GradObj', 'on', 'MaxIter', '100'); set GradObj to on, and max number of iterations to 100
  \%% initialTheta = zeros(2,1)
  \%%  [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options)
  \% u will get [5 5] as the return value of optTheta, functionVal at that point about 0, and exitFlag of 1.
  
Now back to the discussion:
%pre
  In pseudocode, the above is:
  function [jVal, gradient] = costFunction(theta)
  \ jval = [code to compute J(theta)];
  \ gradient(1) = [code to compute d(J(theta))/d(theta-0)];
  \ gradient(2) = [code to compute d(J(theta))/d(theta-1)];
  \ gradient(n+1) = [code to compute d(J(theta))/d(theta-n)];

%pre
  next Video
  Logistic Regression
  Multi-class classification: one vs all
  Example: email foldering/tagging: work, friends, family, hobby
  \                                  y=1,  y=2,     y=3,    y=4
  Medical diagnosis: not ill, cold, flu
  \                   y=1,     y=2,  y=3
  Weather: sunny, cloudy, rainy, snow
  \         y=1,   y=2,    y=3,   y=4
  Suppose you have three class classification problem, you can turn it into three separate binary classification problems
  class 1, 2, and 3. you do: 1 vs rest, 2 vs, rest, and 3 vs rest.
  so for 1 vs rest, reinterpret so that class 1 gets assigned to class 1 or the positive class, and classes 2 and 3 get assigned to class 2, or the negative class.   Now, try to learn a classifier, h(sub theta, super 1)(x)
  do logistic regresssio non it. 
  next do it for class 2, assign class to positive class, and the rest to negative class. and calculate h(sub theta, super 2)(x)
  and do for three: h(sub theta, super 3)(x)
  so we have three classifiers: h(theta; i)(x) = P(y=i|x;theta) for i in 1, 2, 3
  Then, on a new input x, to make a prediction, pick the class i that maximizes
  \ max over i, h(theta, i)(x)
  
%pre
  Next video: Regularization
  The problem of Overfitting
  the example of predicting housing prices wrt the size of the house, or given the size of the house
  theta-0 + theta-1.x   theta-0 + theta-1.x + theta-2.x^2     theta-0 + theta-1.x + theta-2.x^2 + theta-3.x^3 + theta-4.x^4
  the linear one may be underfitting, since it seems from the graph that it does not capture the data very well.
  So we call that this algorithm has "high bias"
  roughly it means that it is not fitting even the training data very well.
  if in example we have 5 training samples and we do the 4th degree polynomial
  we may fit all the points. we may run into the problem of overfitting here.
  This algorithm has "high variance". basically it means we dont have enough data to constrain the set
  of solutions that we can employ to fit the data.
  so overfitting: if we have too many features, the learned hypothesis may fit the data very well(J(theta) very nearly = 0)
  but fail to generalize to new examples (predict prices on new examples).
  the same thing can happen to logistic regression as well.
  
  examples: h(theta, x) = g(theta-0 + theta-1.x1 + theta-2.x2)
  the above may try to partition only using a straight line. this may be an example of underfitting.
  h(theta,x) = g(theta-0 + theta-1.x1 + theta-2.x2 + theta-3.x1^2 + theta-4.x2^2 + theta-5.x1.x2 )
  the above may be just right.
  
  but if we fit very high degree like
  h(theta, x) = g(theta-0 + theta-1.x1 + theta-2.x1^2 + theta-3.x1^2.x2 + theta-4.x1^2.x2^2 + theta-5.x1^2.x2^3 + ...)
  The above may be overfitting.
  How to address Overfitting?
  you can plot the data and see visually, 
  if we have lots of features, and very little training data, then overfitting may be a problem.
  two main options: try to reduce the number of features, manually select which features to keep.
  the other is: model selection algorithms (automatically decide which features to keep and whcih to throw out)
  the second option is "Regularization."
  \- keep all the features, but reduce magnitude/values of parameters theta-j
  \- works well when we have lots of featuers, each of which contributes a bit to predicting y.

%pre
  Regularization
  Cost Function
  Now the next all videos should be done on the course page, not offline due to exercises.
  Intuition. We saw that in linear regression example, 2nd degree polynomial looked just right, but
  higher order one loooked overfitting.
  Suppose we penalize and make theta-3, theta-4 very small. 
  min over theta, (1/2m)(sum i in 1..m)((h(theta, x(i))-y(i))^2)
  now the new one is:
  min over theta, (1/2m)(sum i in 1..m)((h(theta, x(i))-y(i))^2) + 1000.theta-3^2 + 1000.theta-4^2
  in the above cost function, it will be minimized with theta-3, theta-4 close to 0
  so we end up with essentially a quadratic function. we penalized two of the parameter values being large
  Regularization:
  Small values for parameters theta-0,...theta-n
  \- simpler hypothesis
  \- less prone to overfitting.
  also, it is possible to show that having small values of the parameters corresponds to smoother functions
  which are also less prone to overfitting.
  Example. Housing, given x1, x2, ..., x100, and theta-0...theta-100,
  the new cost function becomes:
  J(theta) = (1/2m)[(summation(i in 1..m)(h(xi)-yi)^2)+(lambda)(summation(j in 1..n)theta-j^2)]
  thsi will shrink all parameters. also, we will not penalize theta-0, so the second summation is from 1..n
  we regularize theta-1 to theta-n
  lambda is the regularization parameter.
  it controls tradeoff between two different goals. the first goal is captured by the first term above (the usual one)
  is to fit training data well. the second goal is to keep the parameters small, which is captured by teh regularization term.
  lambda controls the tradeoff between these two goals. 
  question at 7:59
  if we make lambda very large, example 10^10, then all theta-1...theta-n are close to 0, and we are left with essentially
  h(x) = theta-0 + very little else, so we are left with a constant straight line sort of thing
  this is an example of underfitting.
  this hypothesis has a very strong preconception, or high bias, to fit a flat horizontal line
  for regularization to work well, choose a good choice for lambda,
  in the next two videos, we will apply regularization to linear regression and logistic regression.
%pre
  Set 2 Done!
%pre
  Regularization
  Regularized Linear Regression
  Previously, two algos, one based on gradient descent, the other based on the Normal Equation.
  generalize those two to regularized linear regression.
  previously,
  repeat: {
  \ theta-j = theta-j - alpha.1/m.(summation i in 1..m)(h(theta, xi) - y(i))x(j, i)
  \    for j in 0..n
  \ theta-0 = theta-0 - alpha.1/m.(summation i in 1..m)(h(theta, xi) - y(i))x(0, i)
  just write teh update for theta-0 separately
  }
  when we modify the algorithm, we will treat theta-0 differetnly since we dont penalize it.
  Now the above will change to:
  repeat {
  \ theta-0 = theta-0 - alpha.1/m.(summation i in 1..m)(h(theta, xi) - y(i))x(0, i)
  \ theta-0 same as before;
  \ theta-j = theta-j - alpha.[1/m.(summation i in 1..m)(h(theta, xi) - y(i))x(j, i) + lambda/m.theta-j]
  \    for j in 1..n
  }
  basically, the new way to update the thetas is
  from the partial derivative wrt the theta of the new regularized cost function.
  the update can be written equivalently as follows after grouping similar terms:
  theta-j = theta-j(1-alpha.lambda/m) - (alpha/m)(summation i in 1..m)(h(theta, xi) - y(i))x(j,i)
  think of 1-alpha.lambda/m as little less than 1, so the iteration has the effect of shrinking theta-j,
  example theta-j = 0.99.theta-j if 1-alpha.lambda/m is 0.99
  The other algorithm was Normal equation
  X = [x(1)' % each row corresponding to a set of training example
  \    x(2)'
  \     ...
  \    x(m)']
  X is an m by (n+1) dimensional matrix
  Y = [y(1) y(2) ... y(m)]'
  Y is m by 1 column vector
  Theta = (X'X)^(-1)X'Y
  Now, we are also using regularization, so it will change to
  so let u be the following matrix:
  0 0 ...   0
  0 1 ...   0
  0 0 1 ... 0
  \.
  \.
  0 0 ...   1
  u is a n+1 by n+1 dimensional matrix
  the new Theta then is:
  Theta = [X'X + lambda times u]^(-1)X'Y
  lets discuss the noninvertibility issue
  if m <= n
  that is, if #examples <= #features
  in this case, X'X is noninvertible/singular/degenerate
  if u use pinv in octave, it will kind of do the right thing, but may not give a very good hypothesis
  fortunately regularization takes care of this, since u can choose lambda such that [X'X+lambda times u] is invertible
  it is possible to prove that if lambda is greater than 0, the above matrix will not be singular

%pre
  regularized logistic regression
  we talked about gradient descent, and also advanced optimization methods previously
  where we talked of a function that can compute the cost function and the derivatives and using that we
  can implement logistic regression.
  it is also prone to overfitting if high order polynomial terms
  if u have lots of features u can end up with overfitting
  now the new cost function is:
  J(theta) = -[1/m.summation(i in 1..m)(y(i)log h(theta, x(i)) + (1-y(i))(log(1-h(theta, x(i)))))] + lambda/2m.summation(j in 1..n)theta-j^2
  before, our update to theta-j was as follows:
  repeat {
  theta-j = theta-j - alpha.(1/m).summation(i in 1..m)(h(theta, x(i)) - y(i))x(j, i)
  \                                      for j in 0...n
  write for theta-0 separately from the rest.

  new one
  repeat {
  theta-0 = theta-0 - alpha.(1/m).summation(i in 1..m)(h(theta, x(i)) - y(i))x(0, i)
  theta-j = theta-j - alpha.[(1/m).summation(i in 1..m)(h(theta, x(i)) - y(i))x(j, i) + lambda/m.theta-j]
  \
  }                                           for j in 1...n
  question at 3:54
  Now coming to Advanced Optimization Methods
  Now, the updated code is as follows:
  function [jVal, gradient] = costFunctiona(theta)
  \  jVal = (theta(1)-5)^2 + (theta(2)-5)^2 + theta(1)^2 + theta(2)^2;
  \  gradient = zeros(2,1);
  \  gradient(1) = 4*theta(1)-10;
  \  gradient(2) = 4*theta(2)-10;
  \  options = optimset('GradObj', 'on', 'MaxIter', '100'); set GradObj to on, and max number of iterations to 100
  \  initialTheta = zeros(2,1)
  \  [optTheta, functionVal, exitFlag] = fminunc(@costFunctiona, initialTheta, options)
  the answer it returns is 2.5, 2.5 for theta-0, theta-1 respectively.
  The new cost function is as follows:
  J(theta) = [-(1/m).summation(i in 1..m){y(i)log h(theta, x(i)) + (1-y(i))(log(1-h(theta, x(i))))}] + lambda/(2.m).summation(j in 1..n)theta-j^2
  The derivative for theta-0 does not change, for rest it changes:
  repeat {
  partial derivative wrt theta-0: 1/m.summation(i in 1..m)(h(theta, x(i))-y(i))x(0,i)
  partial derivative for others: 
  \  1/m.summation(i in 1..m)((h(theta, x(i))-y(i))x(j,i)) + lambda/m.theta-j
  }
  next, very strong class of non linear classifiers
  }
  
%pre
  27 december friday
  Non Linear Hypothesis
  Neural Networks Representation
  Lots of features, x1..x100, and you want to classify/predict whether the house will be sold in the next 6 months or not.
  Doing logistic regression works well when we have only few features, example x1 and x2, we can
  have high order polynomial terms that can be used to make curved boundaries etc. but for v large number of
  features, if u include all the quadratic terms for x1..x1-00, you will get 4950 terms. roughly n^2 terms
  you can include only a subset of these, example only xi^2 you cant get very interesting hypothesis.
  if you include the cubic ones, you get nearly n^3 terms.
  so it doesnt seem a good way, as n is usually v large for machine learning.
  Computer Vision: car detector
  u want to train a classified to examine an image and decide if it is car or not.
  what is the dimension of the feature space?
  
  a few labeled examples of cars/not cars, then give it to learning algo, which makes a classifier, then give an image of car, and run algo on it.
  even if you have 50px by 50px images, so n=2500 pixels in grayscale, and  n=7500 in rgb,
  if u included all quadratic features, then for 2500px, we end up with about 3 million features per training example.
  so simple logistic regression together with quadratic
  or cubic terms is not feasible to learn complex non linear hypothesis.
  if you include the quadratic features
  you get around 3 million features.
  question at about 8:54
  so neural networks are a much better way if input features is very large.
  even if input feature space is large neural networks is a good approach.

%pre
  Neurons and the Brain
  the original goal was to have machines that can mimic the brain.
  there is a 'one learning algorithm' hypothesis
  the same piece of brain tissue can process sight or sound or touch after rewiring. then may be
  there is one learning algorithm that can process sight or sound or touch.
  if you take pictures with a camera and wire those by electrode to your tongue, you will learn to see
  with your tongue in 10 minutes.
  by snapping fingers u can walk around and not hit a thing, as your brain learns to recognize sonar!
  haptic belt: strap around your waist and have north one buzzing, u can give human a direction sense similar 
  to how birds get sense where north is.
  if u plug a third eye into a frog, frog will learn to use that eye as well.

%pre
  Model Representation 1
  represent neural networks 
  simulating neurons in the brain
  neuron in the brain
  neuron has a cell body, and a number of input wires called dendrites
  neuron has an output wire called axon, to send signals to other neurons
  a neuron is a computational unit
  they communicated with little pulses of electricity
  sends a little pulse of electricity to a different neuron
  send from axon of one to dendrite of another
  we model a neuron as a logistic unit
  input wires
  output some value on output wire
  output is h(theta, x)
  it is = 1/(1+exp(-theta'.x))
  X = [x0 ... x3] theta = [theta-0...theta-3]
  x0 is the bias unit which is always 1
  a neuron is an artificial sigmoid activation function
  the thetas are called the weights of the model
  sometimes called parameters of the model
  a nerual network is a group of this stuck together
  in a neural network the first layer is called input layer
  "the layer of x0, x1, x2 etc is layer 1
  the layer of a(0,2) a(1,2) a(2,2) etc is layer 2
  and so so
  a(i,j) is activation of unit i in layer j.
  Theta(j) where j is in superscript, is the matrix of weights controlling function mapping from layer j to layer j+1 
  final layer is called the output layer"
  the middle layer layer 2 is called the hidden layer
  neural networks can have more than one hidden layer
  if any layer is not input or outpt layer is called hidden layer
  a(1, 2) = g(theta(10, 1)x0 + theta(11, 1)x1 + theta(12, 1)x2 + theta(13,1)x3) # theta(10,1) is theta of (1st row 0th column of matric) in the first column
  in the above, a(1, 2) is the activation of unit 1 in layer 2. it depends on theta(1) Theta in superscript, and inputs a(1) where 1 is in superscript.
  
  a(2,2) = g(theta(20, 1)x0 + theta(21, 1)x1 + theta(22, 1)x2 + theta(23,1)x3)
  a(3,2) = g(theta(30, 1)x0 + theta(31, 1)x1 + theta(32, 1)x2 + theta(33, 1)x3)
  h(theta, x) = a(1,3) = g(theta(10,2)a(0,2) + theta(11,2)a(1,2) + theta(12, 2)a(2,2) + theta(13,2)a(3,2))
  dimension oftheta-1 is 3by4 dimension matric
  This video shows the details of the whole neural network model, so it is v good.
  
  if network has sj units in layer j, s(j+1) units in layer j+1, then theta-j will be of dimendion s(j+1) x (s(j)+1), where theta-j is theta supescript j
  
  in artificial neural network, we model as a logistic unit, the yellow circle is the body of the neuron
  we then feed input via input wires or dendrites, and the output wire is called axon in biology
  h(theta, x) = 1/(1+exp(-theta'.x))
  only input nodes x1..xn are drawn, but x0 is the bias neuron or bias unit, it is always equal to 1.
  the artificial neuron is a sigmoid activation function.
  neural network diagram
  x1 - - - a(1,2) - - - h(theta, x)
  x2 - - - a(2,2) - - - |
  x3 - - - a(3,2) - - - |
  layer1   layer 2      layer 3
  
  a(i,j) is activation of unit i in layer j
  theta(j) = matrix of weights controlling function mapping from layer j to layer j + 1
  theta-1 is a 3 x 4 dimensional matrix
  video at 11:42

%pre
  Model Representation 2
  we will do vectorized implementation, and intuition on why they work.
  
  forward propagation: vectorized implementation
  a(i,j) activation of unit i in layer j.
  theta-j, or theta(,j) or theta(j) where j in superscript is matric od wiehgt from layer j to layer j+1.
  This is the actual one, theta(10,1) is value in the matrix of weights to take input vector and convert to output of first hidden layer, 2nd row row column 1 (1,0) and matric number 1 from layer 1 to layer 2. that is, matric from layer 1 to layer 2, indexed by (1,0).
  a(1, 2) = g(theta(10, 1)x0 + theta(11, 1)x1 + theta(12, 1)x2 + theta(13,1)x3)
  a(2,2) = g(theta(20, 1)x0 + theta(21, 1)x1 + theta(22, 1)x2 + theta(23,1)x3)
  a(3,2) = g(theta(30, 1)x0 + theta(31, 1)x1 + theta(32, 1)x2 + theta(33, 1)x3)
  h(theta, x) = a(1,3) = g(theta(10,2)a(0,2) + theta(11,2)a(1,2) + theta(12, 2)a(2,2) + theta(13,2)a(3,2))
  let a(1,2) = g(z(1,2))
  a(2,2) = g(z(2,2))
  a(3,2) = g(z(3,2))
  X = [x0 ... x3]
  Z(2) = [z(1,2) z(2,2) z(3,2)]
  similarly, denote the whole vector of outputs of hiden layer as a(2) where 2 in superscript. so a(x) = g(z(2)).
  
  
  z(2) = theta(1)X
  a(2) = g(z(2))
  
  The above is the vectorized implementation.
  
  a(2) and z(2) in the above are 3-dim vectors
  we can also define a(1) as X, that is, a(1) is the first column which is of inputs, so 
  z(2) = theta(1)a(1) as a(1) = X, the input vector
  Add a(0,2) = 1
  then z(3) = theta(2)a(2)
  h(theta, x) = a(3) = g(z(3))
  to compute h(theta)
  g(z(3)) = h(theta,x) the output
  for intuition, if you cover up the first layer, it resembles logistic regression
  h(theta, x) = g(theta(1,2)a0(2) + theta(1,2)a1(2) + theta(1,2)a2(2) + theta(1,2)a3(2)) where the 2's are in the superscript
  to denote column 2
  so, instead of feeding input into logistic regression, the neural network computes a's, and feeds these a's, which it learns
  itself, into logistic regression. so in one sense it learns better features to feed to logistic regression than the features
  that were originally given.
  there can be other neural network architectures
  
  Now, in the actual model, that is in the vectorized implementation, we also add an extra term a(0,2) which is always 1.
  so z(,3) = Theta(,2)a(,2)
  h(theta,x) = a(,3) = g(z(,3))
  after adding a(0,2) the a(,2) is in R4 instead of R3.
  This method of computing h from x is called "forward propagation"
%pre
  Examples and Intuitions 1
  Do this again, so no notes this time
  x1 and x2 can take values 0 or 1.
  non linear classification example: xor xnor
  y = x1 xor x2
  x1 xnor x2
  not(x1 xor x2)
  another example: x1, x2 in {0,1}
  compute x1 AND x2
  draw the bias unit as well, the +1 unit.
  +1 with -30, x1 with +20, x2 with +20
  compute h(theta, x) = g(-30 + 20x1 + 20x2)
  g(-30) v close to 0
  if one of htem is 1, then again g(-10) v close to 0
  if both are 1, then g(10) v close to 1
  so this is the AND function of x1 and x2
  
%pre
  Examples and Intuition 2 from the downloaded ones
  negation.
  Not x1.
  input is x1. bias unit is +1
  g(10 - 20x1)
  to do negation, the general idea is to use a large negative value.
  x1 and x2: (-30, 20, 20)
  x1 or x2: (-10, 20, 20)
  not x1: (10, -20)
  not x1 and not x2: (10, -20, -20)
  
%pre
  Multiclass classification
  it is basically an extension of one vs all, if there are ten classes the output has 10 components, so it is a 10-dim object.
  hand digit recognition is an example
  is it a picture of a pedeestrian or a truck or a car (computer vision example)
  this is done with multiple output units: one vs all
  the output is a vector of 4 numbers
  the first output is pedestrian, the second a car(yes or no?) teh third a truck (yes or no?) and so on
  we want if pedestrian output should be [1 0 0  0] and so on.
  training set is (x1,y1) or (x(,1),y(,1)), (x2,y2),(x3,y3),...,(xm,ym)
  previously yi was in {1,2,3,4}, now y(i) is one of [1 0 0 0] [0 1 0 0] [0 0 1 0] [0 0 0 1]
  depending on which of pedestrain, car, truck, motorbike the image is.
  we want to design neural network weights so that h(theta, xi) is approx equal = to yi
  the next video is on: how to automatically learn the parameters of the neural network, that is, the thetas in each layer
  how to given a training set, do the above.

%pre
  Cost Function
  use the same cost function as in logistic regression, normalize over all thetas except theta-0 in each layer. next video on how to optimize the cost function.
  talk about the cost function for fitting the parameters of the neural network.
  L = toatl number of layers
  sl, or s_l: no of units (not counting bias unit) in layer l, so s(l) as well.
  input layer is layer 1.
  Training set: pair of (xi,yi) values, that is, given an input xi, what should be the output yi
  there are two types of classification problems we will cosnider: one is binary classification, output is 0 or 1, the other is multiclass
  classification, where output is [1 0 0 0 ] etc.
  in the multiclass case, the output is in RK, where K is the number of classes to detect.
  The cost function of the neural network is a generalization of the one for logistic regression
  logistic regression:
  J(theta) = (-1/m).[summation(i in 1..m)(yi.log(h(theta, xi)) + (1-yi).log(1-h(theta, xi)))] + (lambda/m).summation(j in 1..n)(theta-j ^ 2)
  in the above, we dont regularoze on theta-0, therefor j starts from 1..n
  for nn, our cost funciton is generalization,	
  termonology:
  h(theta, x) is in Rk, if it is a k-classification problem,
  h(theta, x)_i is the ith output of the output vector, we can denote it other way: (h(theta, x) sub i) 
  The actual nn cost function is below, basically it is the same as above but also summed over the K output units.
  The next video is on how to optimize the cost function.

%pre
  Back Propagation Algorithm
  todo
  minimize the cost function is the goal
  given theta we need code to compute J(theta), and the partial derivatives: (d J(theta))/(d theta(ij, l))
  Lets talk about if we have only one training example, that is, one (xi,yi)
  Question is, how to compute gradient?
  what is the sequence of calculations we will do with this one training example?
  first step: do forward propagation to compute each of teh a's in all layers, as well as the output.
  here it is:
  a(1) = x
  z(2) = theta(1)a(1)
  a(2) = g(z(2)) (add a(0,2))
  z(3) = theta(2)a(2)
  a(3) = g(z(3)) add a(0,3)
  z(4) = theta(3)a(3)
  a(4) = g(z(4))
  which isthe final output
  corresponding to 2 hidden layers, 1 input and one output layer with 4 outputs, so total of 4 layers
  In order to compute derivatives, we will use algorithm called backpropagation.
  the intuition is, for each node we will compute delta(j,l) which is error of node j in layer l.
  delat(j,l) = error of node j in layer l
  For each output unit in layer L = 4 in the example
  compute
  delta(j,4) = a(j,4) - y(j)
  a(j,4) can also be written as h(theta, x)_j for the jth value of the vector h(theta, x)
  vectorized implementation is
  delat(4) = a(,4) - y
  where each term above is vector
  next, compute delta terms for the earlier layers like so:
  delta(3) = (Theta-3)T.delta(4).*g'(z(3)) # where .* is the element wise multiplication
  delta(2) = (Theta-2)T.delta(3).*g'(z(2))
  In the above, z(3) = a(3).*(1-a(3)) and so on from calculus
  No delta(1) term
  we only have delta terms for layers 2,3,4 in the example
  It is also possible to prove that: if the lambdas are all 0, that is, no regularization, then
  (d j(theta))/(d theta(ij,l)) = a(j,l)delta(i, l+1)
  Next, what happens when we have a large training set?
  suppose we have m examples in training set: {(x1,y1),(x2,y2),..,(xm,ym)}
  set Delta(ij,l) = 0 for all values of i,j,l # this is upper case delta
  eventually the Delta(ij,l) will be used to compute the partial terms d/dtheta(ij,l) of J(theta)
  For i in 1..m do
  \ set a(1) = x(i)
  \ perform forward propagation to compute a(l) for l=2,3,...,L
  \ Using y(i), compute delta(L) = a(L)-y(i)
  \ Compute delta(L-1), delta(L-2),..., delta(2)
  \ Delta(ij,l) = = Delta(ij,l) + a(j,l)delta(i,l+1)
  end
  The last line above, can be written in matrix form as follows:
  Delta(l) = Delta(l) + delta(l+1).(a(l)T)
  Using the above, compute:
  D(ij,l) = (1/m)Delta(ij,l)) + lambda.Theta(ij,l) if j not equal to 0
  D(ij,l) = (1/m)Delta(ij,l)) if j equal to 0 # this corresponds to the bias term
  it is possible to show that:
  D(ij,l) = d/d(Theta(ij,l)) of J(Theta)
  
  
%pre
  Backpropagation Intuition
  J(Theta) = cost function = -(1/m).[sum(i in 1..m)sum(k in 1..K){y(k,i)log h(theta, x(i))_k + (1-y(k,i))log(1-h(theta, x(i))_k) }]+(lambda/2m).[summation(l in 1..L-1)summation(i in 1..s(l))summation(j in 1..s(l)+1))(Theta(ji,l)^2)]
  the second term which sums over thetas, dont sum over the bias values
  so dont sum over i=0
  Want minimum J(Theta)
  Need to compute
  - J(Theta)
  - (d/(d(theta(ij, l))))J(Theta)
  Input layer is layer 1, second layer is layer 2, and so on.
  mechanical steps intuition
  
%pre
  Implementation Note: Unrolling parameters
  todo
%pre
  Gradient Checking
  Backpropagation can have many errors, and cost function may end up decreasing despite bugs
  can have high level of errors
  there is an idea called gradient checking that eliminates some errors
  it eliminates all problems
  this also ensures the derivatives are right
  numerical estimation of gradients
  set epsilon to 10^-4
  we do the 2-sided difference, so (theta+eps - theta-eps), instead of (theta+eps-theta)
  parameter vector theta
  d/dtheta-1 of J(theta) = J(theta1 + eps, theta2, ..., theta-n) - J(theta1 - eps, theta2, ..., theta-n)/(2.eps)
  and so on
  d/dtheta-n of J(theta) = J(theta1, theta2, ..., theta-n + eps) - J(theta1, theta2, ..., theta-n - eps)/(2.eps)
  and so on
  so we implement the following octave code:
%pre
  for i = 1:n;
  \  thetaPlus = theta;
  \  thetaPlus(i) = thetaPlus(i) + EPSILON; % that is: [theta-1 theta-2 ... theta-i+eps ... theta-n]
  \  thetaMinus = theta;
  \  thetaMinus(i) = thetaMinus(i) - EPSILON;
  \  gradApprox(i) = (J(thetaPlus)-J(thetaMinus))/(2*EPSILON);
  end
%pre
  next, check that gradApprox is approx equal to DVec from backprop.
  if yes, swithc off gradient checking and use the DVec backprop for learning
  gradient checking is slow, so turn it off
%pre
  Random Initialization
  need to pick iniital value of theta
%pre
  optTheta = fminunc(@costFunction, initialTheta, options)
%pre
  do we set initialTheta = zeros(n,1)?
  the above works for logistic regression, but not for nn
  after each update, parameters corresponding to inputs going into each of two hidden units are identical.
  a(1,2) = a(2,2)
  delta(1,2) = delta(2,2)
  d/d theta(01, 1) = d/d theta(02, 1)
  and so on
  so the blue weights are the same, the green weights are the same and so on
  This problem is called the problem of symmetric wieghts
  initialize theta(ij,l) to a random value is (-eps,eps)
%pre
  Theta1 = rand(10,11)*(2*INIT_EPSILON)-INIT_EPSILON; % rand(10,11) is a 10 by 11 matrix each value between 0 and 1 randomly
  Theta2 = rand(1,11)*(2*INIT_EPSILON)-INIT_EPSILON;
%pre
  so this is called symmetry breaking
  
